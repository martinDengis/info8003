{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ae4bfb",
   "metadata": {},
   "source": [
    "# TP1 INFO8003\n",
    "The idea behind this notebook is to get familiar with basic concepts of reinforcement learning such as the design of environments and the state value function $V^\\pi(s)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74ac705",
   "metadata": {},
   "source": [
    "## Question 1: Implementing a Discrete Gold Miner Environment\n",
    "\n",
    "Your task is to create a discrete environment for a \"Gold Miner\" game following the [Gym API](https://gymnasium.farama.org/v0.26.3/api/env/). Specifically, you need to implement the `step` and `reset` functions:\n",
    "\n",
    "- **`step(action)`**:  \n",
    "  This method updates the environment based on the given action. It should return:  \n",
    "  1. The agent's next observation.  \n",
    "  2. The reward obtained for the action.  \n",
    "  3. Whether the episode has ended (`terminated`) or paused (`truncated`).  \n",
    "  4. Additional information, such as metrics or debugging data about the environment.  \n",
    "\n",
    "- **`reset()`**:  \n",
    "  This method initializes the environment to its starting state. It should return:  \n",
    "  1. The agent's first observation for a new episode.  \n",
    "  2. Additional information, such as metrics or debugging data.  \n",
    "\n",
    "---\n",
    "\n",
    "### Gold Miner Environment\n",
    "\n",
    "You get to visit a gold mine and you are allowed to spend as much time in it as you wish to get as much gold as possible. Working and spending time in the mine is complicated and rather than maximizing the quantity of gold mined, you want to optimize your satisfaction. You don't plan to stay in the mine for very long. Since you have followed the RL course last year, you have computed a discount factor of $95\\%$ which allows you to not spend too much time in the gold mine.\n",
    "\n",
    "You start at the entry of the mine, at the ground level. You can decide to either go one level down, stay where you are or go one level up. To get to the gold, you have to dig trough several layers of soil. Staying at ground level, does not give you any satisfaction. The first two layers cost you one point of satisfaction each. The third layer contains a little gold and rewards you with one satisfaction point. If you decide to dig deeper, the next 4 layers are more complicated. Each layer further would respectively cost you 1,2,3 and 4 satisfaction points. However, the last layer is full of gold and digging there rewards you with 10 satisfaction points. If you decide to stay on a layer, you keep digging the same layer and it provides the same satisfaction over and over again.\n",
    "\n",
    "![Gold Miner Environment](Mine.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ba1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Tuple\n",
    "\n",
    "class MinerGymEnv(Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the Gold Miner environment.\n",
    "        \"\"\"\n",
    "        self.action_space = Discrete(n=3, start=-1)\n",
    "        self.observation_space = Discrete(9)\n",
    "        self.current_state = 0\n",
    "        self.discount_factor = 0.95\n",
    "        self.reward = [0, -1, -1, 1, -1, -2, -3, -4, 10]    # Reward for each state by index\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform an action in the environment and update its state.\n",
    "\n",
    "        Parameters:\n",
    "        action (int): The action taken by the agent. This could be moving up, down, or staying at the current level.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - next_state (int): The new state of the environment after the action.\n",
    "            - reward (float): The reward received for taking the action.\n",
    "            - terminated (bool): Whether the episode has ended.\n",
    "            - truncated (bool): Whether the episode was truncated (paused).\n",
    "            - info (dict): Additional information, such as metrics or debugging data.\n",
    "        \"\"\"\n",
    "        next_state = min(8, max(0, self.current_state + action))\n",
    "        if next_state not in self.observation_space:\n",
    "            raise ValueError(\"Invalid action: the action should lead to a valid state in the environment.\")\n",
    "\n",
    "        reward = self.reward[next_state]\n",
    "        terminated = next_state == 8\n",
    "        truncated = False\n",
    "        self.current_state = next_state\n",
    "\n",
    "        return next_state, reward, terminated, truncated, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to its initial state.\n",
    "\n",
    "        This method is used to start a new episode by resetting the environment's state\n",
    "        to the initial state (ground level).\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - current_state (int): The initial state of the environment.\n",
    "            - info (dict): Additional information, such as metrics or debugging data.\n",
    "        \"\"\"\n",
    "        self.current_state = 0\n",
    "\n",
    "        return self.current_state, {}\n",
    "\n",
    "    def value_function(self, agent, n):\n",
    "        \"\"\"\n",
    "        Estimate the state value function for a given policy over a specified number of iterations.\n",
    "\n",
    "        Parameters:\n",
    "        agent (Agent): The agent whose policy is used to estimate the value function.\n",
    "        n (int): The number of iterations to perform for estimating the value function.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray: An array representing the estimated value of each state, indicating the expected cumulative reward for following the policy from each state.\n",
    "        \"\"\"\n",
    "        state_value_function_table = np.zeros(9)\n",
    "        for _ in range(n):\n",
    "            for state in range(9):\n",
    "                if state == 8:\n",
    "                    state_value_function_table[state] = self.reward[state] # Reward = 10\n",
    "                else:\n",
    "                    policy = agent.chose_action(state)\n",
    "                    next_state = state + policy\n",
    "                    if 0 <= next_state < len(state_value_function_table):\n",
    "                        state_value_function_table[state] = self.reward[state] + self.discount_factor * state_value_function_table[next_state]\n",
    "                    else:\n",
    "                        state_value_function_table[state] = self.reward[state] # Handle out-of-bounds by only considering the immediate reward\n",
    "\n",
    "        return state_value_function_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c2a19",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "You are asked to code an agent that choses the always-dig policy via the function: \n",
    "- chose_action(state): that takes a state as input and returns an always-dig action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd38d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "        \"\"\"\n",
    "        self.action = 1\n",
    "\n",
    "    def chose_action(self,state):\n",
    "        \"\"\"\n",
    "        Determine the action to take based on the current state.\n",
    "\n",
    "        Parameters:\n",
    "        state (int): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        int: The action chosen by the agent. In this implementation, the agent always returns\n",
    "        a predefined policy of always digging.\n",
    "        \"\"\"\n",
    "        return self.action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66941db",
   "metadata": {},
   "source": [
    "### Test your implementation\n",
    "\n",
    "Interaction between agent and domain over 10 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f299407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, -1, 1)\n",
      "(1, 1, -1, 2)\n",
      "(2, 1, 1, 3)\n",
      "(3, 1, -1, 4)\n",
      "(4, 1, -2, 5)\n",
      "(5, 1, -3, 6)\n",
      "(6, 1, -4, 7)\n",
      "(7, 1, 10, 8)\n",
      "(8, 1, 10, 8)\n",
      "(8, 1, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = MinerGymEnv()\n",
    "state, info = env.reset()\n",
    "agent = Agent()\n",
    "steps = 10\n",
    "trajectory = []\n",
    "\n",
    "for _ in range(steps):\n",
    "    action = agent.chose_action(state)\n",
    "    next_state, reward, _, _, _ = env.step(action)\n",
    "    trajectory.append((state, action, reward, next_state))\n",
    "    state = next_state\n",
    "\n",
    "for i, sample in enumerate(trajectory):\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0607d75",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Implement the state value function that evalues the estimated reward if a policy is applied for all states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d78a4a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.72161367, -1.81222491, -0.85497359,  0.15265937, -0.8919375 ,\n",
       "        0.11375   ,  2.225     ,  5.5       , 10.        ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.value_function(agent, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de0ebbc",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "You will find here below the implementation of the grid world environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94ab3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(Env):\n",
    "    def __init__(self, grid_rewards, state_0= (3, 0), deterministic=True):\n",
    "        \"\"\"\n",
    "        Initialize the GridWorld environment.\n",
    "\n",
    "        Parameters:\n",
    "        grid_rewards (list of lists): A 2D list representing the reward values for each cell in the grid.\n",
    "        state_0 (tuple): The initial state of the agent in the grid, default is (3, 0).\n",
    "        deterministic (bool): If True, the environment behaves deterministically; otherwise, it behaves stochastically.\n",
    "        \"\"\"\n",
    "        self.grid = np.array(grid_rewards)\n",
    "        self.n, self.m = self.grid.shape\n",
    "        self.action_space = Discrete(4)  # Actions: 0=Right, 1=Left, 2=Up, 3=Down\n",
    "        self.observation_space = Tuple((Discrete(self.n), Discrete(self.m)))\n",
    "        self.state = state_0\n",
    "        self.deterministic = deterministic\n",
    "        self.gamma=0.99\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute an action in the environment and update the agent's state.\n",
    "\n",
    "        Parameters:\n",
    "        action (int): The action to be taken by the agent. Actions are encoded as integers:\n",
    "                    0=Right, 1=Left, 2=Up, 3=Down.\n",
    "\n",
    "        Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - next_state (tuple): The new state of the agent after the action.\n",
    "            - reward (float): The reward received for taking the action.\n",
    "            - terminated (bool): Always False, as the environment does not have terminal states.\n",
    "            - truncated (bool): Always False, as the environment does not have truncated states.\n",
    "            - info (dict): An empty dictionary for additional information.\n",
    "        \"\"\"\n",
    "        actions = {\n",
    "            0: (0, 1),   # Right\n",
    "            1: (0, -1),  # Left\n",
    "            2: (-1, 0),  # Up\n",
    "            3: (1, 0)    # Down\n",
    "        }\n",
    "\n",
    "        i, j = actions[action]\n",
    "        x, y = self.state\n",
    "\n",
    "        if self.deterministic:\n",
    "            next_state = (\n",
    "                min(max(x + i, 0), self.n - 1),\n",
    "                min(max(y + j, 0), self.m - 1)\n",
    "            )\n",
    "        else:\n",
    "            if np.random.uniform() <= 0.5:\n",
    "                next_state = (\n",
    "                    min(max(x + i, 0), self.n - 1),\n",
    "                    min(max(y + j, 0), self.m - 1)\n",
    "                )\n",
    "            else:\n",
    "                next_state = (0, 0)\n",
    "\n",
    "        reward = self.grid[next_state]\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, False, False, {}\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to the initial state.\n",
    "\n",
    "        Returns:\n",
    "        tuple: The initial state of the environment.\n",
    "        \"\"\"\n",
    "        self.state = (3, 0)\n",
    "        return self.state\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Display the current state of the grid with the agent's position marked.\n",
    "\n",
    "        The grid is displayed with '.' for empty cells and 'A' for the agent's current position.\n",
    "        \"\"\"\n",
    "        grid_display = np.zeros_like(self.grid, dtype=str)\n",
    "        grid_display[:] = '.'\n",
    "        x, y = self.state\n",
    "        grid_display[x, y] = 'A'\n",
    "        print('\\n'.join(' '.join(row) for row in grid_display))\n",
    "\n",
    "grid_rewards = [\n",
    "    [-3, 1, -5, 0, 19],\n",
    "    [6, 3, 8, 9, 10],\n",
    "    [5, -8, 4, 1, -8],\n",
    "    [6, -9, 4, 19, -5],\n",
    "    [-20, -17, -4, -3, 9]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63250354",
   "metadata": {},
   "source": [
    "Implement the rule-based policy \"always go right\". Simulate the policy in the domain through a single trajectory of 10 steps, starting by the initial state s0 = (3, 0). Display the trajectories as a sequence of four tuples (s0, a0, r0, s1), ... , (s10, a10, r10, s11)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee31ffca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: (3, 0), Action: 0, Reward: -9, Next State: (3, 1)\n",
      "State: (3, 1), Action: 0, Reward: 4, Next State: (3, 2)\n",
      "State: (3, 2), Action: 0, Reward: 19, Next State: (3, 3)\n",
      "State: (3, 3), Action: 0, Reward: -5, Next State: (3, 4)\n",
      "State: (3, 4), Action: 0, Reward: -5, Next State: (3, 4)\n",
      "State: (3, 4), Action: 0, Reward: -5, Next State: (3, 4)\n",
      "State: (3, 4), Action: 0, Reward: -5, Next State: (3, 4)\n",
      "State: (3, 4), Action: 0, Reward: -5, Next State: (3, 4)\n",
      "State: (3, 4), Action: 0, Reward: -5, Next State: (3, 4)\n",
      "State: (3, 4), Action: 0, Reward: -5, Next State: (3, 4)\n"
     ]
    }
   ],
   "source": [
    "def rule_based_policy(state):\n",
    "    \"\"\"\n",
    "    Define a rule-based policy for the agent.\n",
    "\n",
    "    Parameters:\n",
    "    state (tuple): The current state of the environment.\n",
    "\n",
    "    Returns:\n",
    "    int: The action chosen by the policy.\n",
    "    \"\"\"\n",
    "    return 0\n",
    "\n",
    "def simulate_policy(env, policy, steps=10):\n",
    "    \"\"\"\n",
    "    Simulate a policy in the given environment over a specified number of steps.\n",
    "\n",
    "    Parameters:\n",
    "    env (Env): The environment in which the policy is to be simulated.\n",
    "    policy (function): A function that takes a state as input and returns an action.\n",
    "    steps (int): The number of steps to simulate the policy for.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples representing the trajectory, where each tuple contains:\n",
    "          - state (tuple): The state before taking the action.\n",
    "          - action (int): The action taken.\n",
    "          - reward (float): The reward received after taking the action.\n",
    "          - next_state (tuple): The state after taking the action.\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    trajectory = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        a = policy(s)\n",
    "        s_next, r, _, _, _ = env.step(a)\n",
    "        trajectory.append((s, a, r, s_next))\n",
    "        s = s_next\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "env = GridWorld(grid_rewards, deterministic=True)\n",
    "# Simulate trajectory\n",
    "trajectory = simulate_policy(env, rule_based_policy, steps=10)\n",
    "for t in trajectory:\n",
    "    s, a, r, s_next = t\n",
    "    print(f\"State: {s}, Action: {a}, Reward: {r}, Next State: {s_next}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1a2b3",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Implement a routine to estimate $V^μ_N$ in the deterministic domain, where $\\mu : S \\rightarrow A$ is a stationary policy. Test your implementation with your rule-based policy of Question 4. \n",
    "\n",
    "Provide a bound on the suboptimality of $\\mu^*_{100}$ with respect to $µ^*$. Is the bound a good\n",
    "one? Compute the value of N such that this bound is equal to 0.01. Motivate your choice using the discount factor $\\gamma$.\n",
    "\n",
    "Display $V^μ_N(s)$ for each state $s$.\n",
    "\n",
    "Reminder: $V^μ_N(s) = r(s,a) + \\gamma * V^μ_{N-1}(f(s, a))\\quad \\forall N \\ge 1$ with $V^μ_0(s) = 0$ in a deterministic domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82b43902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated V with N=100:\n",
      "[[-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-318.81399107 -318.81399107 -318.81399107 -318.81399107 -318.81399107]]\n"
     ]
    }
   ],
   "source": [
    "def truncated_value_function(env, policy, gamma=0.99, N=100):\n",
    "    \"\"\"\n",
    "    Estimate the state value function using a truncated approach over a specified number of iterations.\n",
    "\n",
    "    This function calculates the expected value of each state in the environment when following a given policy,\n",
    "    using a deterministic approach to account for the environment's dynamics.\n",
    "\n",
    "    Parameters:\n",
    "    env (Env): The environment in which the policy is evaluated.\n",
    "    policy (function): A function that takes a state as input and returns an action according to the policy being evaluated.\n",
    "    gamma (float, optional): The discount factor, which determines the present value of future rewards. Default is 0.99.\n",
    "    N (int, optional): The number of iterations to perform for estimating the value function. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: An array representing the estimated value of each state, indicating the expected cumulative reward for following the policy from each state.\n",
    "    \"\"\"\n",
    "    state_value_function_table = np.zeros((env.n, env.m))\n",
    "    for _ in range(N):\n",
    "        # Update value for each state\n",
    "        for i in range(env.n):\n",
    "            for j in range(env.m):\n",
    "                s = (i, j)\n",
    "                a = policy(s)\n",
    "                s_next, r, _, _, _ = env.step(a)\n",
    "                x, y = s_next\n",
    "                state_value_function_table[i, j] = r + gamma * state_value_function_table[x, y]\n",
    "\n",
    "    return state_value_function_table\n",
    "\n",
    "env.reset()\n",
    "truncated_value_function_table = truncated_value_function(env, rule_based_policy)\n",
    "print(\"Truncated V with N=100:\")\n",
    "print(truncated_value_function_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc7cb1b",
   "metadata": {},
   "source": [
    "> **Answer:** The suboptimality of the truncated value function $V^{\\mu^*_{100}}$ with respect to the true value function $V^{\\mu^*}$ can be bounded by the following inequality:\n",
    ">\n",
    "> $||V^{\\mu^*} - V^{\\mu^*{100}}||_\\infty \\leq \\frac{2 \\gamma^N B_r}{(1 - \\gamma)^2}$\n",
    ">\n",
    "> This bound indicates that the difference between the true value function and the truncated value function decreases exponentially with the number of iterations (N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1fbc812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bound: 279.0\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.95\n",
    "B_r = 20 # suppose reward is bounded by 20\n",
    "\n",
    "# => error_bound = 0.01\n",
    "# <=> N = \\frac{\\ln(\\frac{0.01(1-\\gamma)^2}{2B_r})}{\\ln(\\gamma)}\n",
    "\n",
    "N = np.log((0.01 * (1 - gamma)**2) / (2 * B_r)) / np.log(gamma)\n",
    "N = np.ceil(N)\n",
    "print(\"Bound:\", N)\n",
    "\n",
    "# Else:\n",
    "# N = 0\n",
    "# error_bound = 2 * gamma**N * B_r / ((1 - gamma)**2)\n",
    "\n",
    "# # Find N such that the error bound is equal to 0.01\n",
    "# while error_bound > 0.01:\n",
    "#     N += 1\n",
    "#     error_bound = 2 * gamma**N * B_r / ((1 - gamma)**2)\n",
    "\n",
    "# print(\"Bound:\", N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe96f1",
   "metadata": {},
   "source": [
    "> **Answer:** The bound here is good because it shows that the error decreases exponentially with $N$. However, it will depend on the value of $\\gamma$. In fact, the discount factor $\\gamma$ plays a crucial role in determining the convergence rate of the value function. \n",
    ">\n",
    "> - A higher $\\gamma$ means future rewards are valued more, leading to slower convergence and requiring more iterations to achieve a small error. \n",
    "> - A lower $\\gamma$ values immediate rewards more, leading to faster convergence.\n",
    ">\n",
    "> Choosing $\\gamma = 0.95$ provides a reasonable bound ($N=279$). On the contrary, setting $\\gamma = 0.99999$ leads to $N = 3131975$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44846186",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "Estimate the expectation of the state value function with 10 runs using Monte-carlo estimation.\n",
    "\n",
    "Reminder: $V^μ_N(s) = \\mathbb{E}_w\\{r(s,a,w) + \\gamma * V^μ_{N-1}(f(s,a,w))\\} \\quad \\forall N \\ge 1$ with $V^μ_0(s) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67878cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated V with N=100:\n",
      "[[-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]]\n",
      "[[5.68434189e-14 5.68434189e-14 5.68434189e-14 5.68434189e-14\n",
      "  5.68434189e-14]\n",
      " [5.68434189e-14 5.68434189e-14 5.68434189e-14 5.68434189e-14\n",
      "  5.68434189e-14]\n",
      " [5.68434189e-14 5.68434189e-14 5.68434189e-14 5.68434189e-14\n",
      "  5.68434189e-14]\n",
      " [5.68434189e-14 5.68434189e-14 5.68434189e-14 5.68434189e-14\n",
      "  5.68434189e-14]\n",
      " [5.68434189e-14 5.68434189e-14 5.68434189e-14 5.68434189e-14\n",
      "  5.68434189e-14]]\n"
     ]
    }
   ],
   "source": [
    "def truncated_value_function_stochastic(env, policy, gamma=0.99, N=100):\n",
    "    \"\"\"\n",
    "    Estimate the state value function using a stochastic approach over a specified number of iterations.\n",
    "    This function calculates the expected value of each state in the environment when following a given policy,\n",
    "    using a Monte Carlo method to account for stochasticity in the environment.\n",
    "\n",
    "    Parameters:\n",
    "        env (Env): The environment in which the policy is evaluated.\n",
    "        policy (function): A function that takes a state as input and returns an action according to the policy being evaluated.\n",
    "        gamma (float, optional): The discount factor, which determines the present value of future rewards. Default is 0.99.\n",
    "        N (int, optional): The number of iterations to perform for estimating the value function. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - state_value_function_table (np.ndarray): An array representing the estimated value of each state.\n",
    "            - std_state_value_function_table (np.ndarray): An array representing the standard deviation of the estimated value for each state.\n",
    "    \"\"\"\n",
    "    state_value_function_table = np.zeros((env.n, env.m))\n",
    "    std_state_value_function_table = np.zeros((env.n, env.m))\n",
    "\n",
    "    num_runs = 10  # Monte Carlo runs per state\n",
    "\n",
    "    # Perform N iterations of value function updates\n",
    "    for _ in range(N):\n",
    "        new_value_table = np.zeros_like(state_value_function_table)\n",
    "        new_std_table = np.zeros_like(std_state_value_function_table)\n",
    "\n",
    "        # Loop over all states in the grid\n",
    "        for i in range(env.n):\n",
    "            for j in range(env.m):\n",
    "                s = (i, j)\n",
    "                returns = []\n",
    "\n",
    "                # Run Monte Carlo sampling (10 runs) for each state s\n",
    "                for _ in range(num_runs):\n",
    "                    a = policy(s)\n",
    "                    s_next, r, _, _, _ = env.step(a)\n",
    "                    x, y = s_next\n",
    "                    value_next = state_value_function_table[x, y]\n",
    "                    returns.append(r + gamma * value_next)\n",
    "\n",
    "                # Estimate the expected value and its standard deviation from the 10 samples\n",
    "                new_value_table[i, j] = np.mean(returns)\n",
    "                new_std_table[i, j] = np.std(returns)\n",
    "\n",
    "        # Update the value function table for the next iteration\n",
    "        state_value_function_table = new_value_table\n",
    "        std_state_value_function_table = new_std_table\n",
    "\n",
    "    return state_value_function_table, std_state_value_function_table\n",
    "\n",
    "env = GridWorld(grid_rewards, deterministic=True)\n",
    "truncated_value_function_table, std_truncated_value_function_table = truncated_value_function_stochastic(env, rule_based_policy)\n",
    "print(\"Truncated V with N=100:\")\n",
    "print(truncated_value_function_table)\n",
    "print(std_truncated_value_function_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ed197",
   "metadata": {},
   "source": [
    "Is your bound still valid using Monte Carlo estimation for the expectation ? Why ?\n",
    "\n",
    "> **Answer:** The bound is derived assuming that the expectation is computed exactly. However, when using MC estimates, we actually introduce additional estimation error (variance) because we are approximating the true expectation with a finite number of samples. However, as the number of MC samples increases, the estimate converges to the true expectation, and the bound will hold asymptotically. But here, we only do 10 MC runs which we can assume to be clearly insufficient, so we have to account for the extra error due to sampling variance.\n",
    "> \n",
    "> So **no**, the bound is not valid anymore.\n",
    "\n",
    "## Bonus: \n",
    "How can you adjust your implementation if you know the dynamics of the environment to compute the exact $V^\\mu_N$ function ?\n",
    "Implement your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06a1e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated V with N=100:\n",
      "[[-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]\n",
      " [-316.98382936 -316.98382936 -316.98382936 -316.98382936 -316.98382936]]\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def truncated_value_function(env, policy, gamma=0.99, N=100):\n",
    "    \"\"\"\n",
    "    Estimate the state value function in the stochastic grid world environment over a specified number of iterations.\n",
    "\n",
    "    This function calculates the expected value of each state in the environment when following a given policy,\n",
    "    using the dynamics of the environment to account for stochasticity in the environment.\n",
    "\n",
    "    Parameters:\n",
    "    env (Env): The environment in which the policy is evaluated.\n",
    "    policy (function): A function that takes a state as input and returns an action according to the policy being evaluated.\n",
    "    gamma (float, optional): The discount factor, which determines the present value of future rewards. Default is 0.99.\n",
    "    N (int, optional): The number of iterations to perform for estimating the value function. Default is 100.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - state_value_function_table (np.ndarray): An array representing the estimated value of each state.\n",
    "        - std_state_value_function_table (np.ndarray): An array representing the standard deviation of the estimated value for each state.\n",
    "    \"\"\"\n",
    "    state_value_function_table = np.zeros((env.n, env.m))\n",
    "    std_state_value_function_table = np.zeros((env.n, env.m))\n",
    "\n",
    "    for _ in range(N):\n",
    "        new_state_value_function_table = np.zeros((env.n, env.m))\n",
    "        for i in range(env.n):\n",
    "            for j in range(env.m):\n",
    "                s = (i, j)\n",
    "                a = policy(s)\n",
    "                s_next, r, _, _, _ = env.step(a)\n",
    "                x, y = s_next\n",
    "                value = r + gamma * state_value_function_table[x, y]\n",
    "                new_state_value_function_table[i, j] = value\n",
    "        state_value_function_table = new_state_value_function_table\n",
    "\n",
    "    return state_value_function_table, std_state_value_function_table\n",
    "\n",
    "# Example usage\n",
    "truncated_value_function_table, std_truncated_value_function_table = truncated_value_function(env, rule_based_policy)\n",
    "print(\"Truncated V with N=100:\")\n",
    "print(truncated_value_function_table)\n",
    "print(std_truncated_value_function_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
