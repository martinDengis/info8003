{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4 INFO8003\n",
    "The idea behind this notebook is to get familiar with the K-bandits problem and advanced Q-learning with deep Q network (DQN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: K-bandits problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desciption of the problem \n",
    "\n",
    "In this first part, we will study the K-armed bandit problem provided here below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT CHANGE \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class KArmedBandit:\n",
    "    def __init__(self, number_of_arms, true_rewards, width):\n",
    "        self.number_of_arms = number_of_arms\n",
    "        assert len(true_rewards) == number_of_arms\n",
    "        self.true_rewards = true_rewards\n",
    "        assert len(width) == number_of_arms\n",
    "        self.width = width\n",
    "        self.arm_counts = np.zeros(number_of_arms)\n",
    "        self.cumulative_reward = 0\n",
    "\n",
    "    def pull_arm(self, k):\n",
    "        reward = np.random.uniform(self.true_rewards[k]- self.width[k]/2, self.true_rewards[k] + self.width[k]/2) \n",
    "        self.arm_counts[k] += 1\n",
    "        self.cumulative_reward += reward\n",
    "        return reward\n",
    "    \n",
    "    def get_cummulative_reward(self):\n",
    "        return self.cumulative_reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cumulative_reward = 0\n",
    "        self.arm_counts = np.zeros(number_of_arms)\n",
    "\n",
    "# Parameters\n",
    "number_of_arms = 4\n",
    "true_rewards = np.linspace(0.4, 0.46, number_of_arms)\n",
    "width = np.array([0.2] * number_of_arms)\n",
    "\n",
    "bandit = KArmedBandit(number_of_arms, true_rewards, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try a random strategy to pull arms and observe the behavior by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "\n",
    "for i in range(num_steps):\n",
    "    reward = bandit.pull_arm(np.random.choice(number_of_arms))\n",
    "print(f\"Rewards collected: {bandit.cumulative_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another exploration strategy seen in the course is the epsilon-greedy. Try the following code to run this strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, number_of_arms, epsilon):\n",
    "        self.number_of_arms = number_of_arms\n",
    "        self.epsilon = epsilon\n",
    "        self.arm_values = np.zeros(number_of_arms)\n",
    "        self.arm_counts = np.zeros(number_of_arms)\n",
    "\n",
    "    def choose_arm(self):\n",
    "        for k, arm_count in enumerate(self.arm_counts):\n",
    "            if arm_count == 0:\n",
    "                return k\n",
    "        return np.random.choice([np.argmax(self.arm_values), np.random.choice(number_of_arms)], p=[1-self.epsilon, self.epsilon])\n",
    "    \n",
    "    def update(self, reward, arm_selected):\n",
    "        self.epsilon *= 0.99\n",
    "        self.arm_counts[arm_selected] += 1\n",
    "        self.arm_values[arm_selected] = (1 - 1 / self.arm_counts[arm_selected]) * self.arm_values[arm_selected] + 1 / self.arm_counts[arm_selected] * reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.arm_values = np.zeros(number_of_arms)\n",
    "\n",
    "    def get_arm_counts(self):\n",
    "        return self.arm_counts.copy()\n",
    "\n",
    "# Parameters\n",
    "number_of_arms = 4\n",
    "true_rewards = np.linspace(0.4, 0.46, number_of_arms)\n",
    "width = np.array([0.2] * number_of_arms)\n",
    "\n",
    "# Run experiment\n",
    "bandit = KArmedBandit(number_of_arms, true_rewards, width)\n",
    "epsilon = 0.1\n",
    "eps = EpsilonGreedy(number_of_arms, epsilon)\n",
    "num_steps = 1000\n",
    "arm_counts_history = []\n",
    "\n",
    "for i in range(num_steps):\n",
    "    j = eps.choose_arm()\n",
    "    reward = bandit.pull_arm(j)\n",
    "    eps.update(reward, j)\n",
    "    current_result = eps.get_arm_counts()\n",
    "    arm_counts_history.append(current_result)\n",
    "print(f\"Arm counters: {eps.get_arm_counts()}\")\n",
    "print(f\"Arm values: {eps.arm_values}\")\n",
    "print(f\"Rewards collected: {bandit.cumulative_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "During the course, we have seen an algorithm called Upper Confidence Bound (UCB) Algorithm. Code the UCB algorithm following the same API than the EpsilonGreedy class to solve the following K-armed bandit problem. The pseudo code here below is given to help you.\n",
    "\n",
    "\n",
    "### Upper Confidence Bound (UCB) Algorithm Pseudo Code\n",
    "\n",
    "#### Input:\n",
    "- `K`: Number of arms (actions)\n",
    "- `N`: Total number of rounds (iterations)\n",
    "\n",
    "##### Initialize:\n",
    "- `counts[k]` = 0 for all `k` in `[1, K]` (number of times arm `k` has been selected)\n",
    "- `values[k]` = 0 for all `k` in `[1, K]` (sum of rewards obtained from arm `k`)\n",
    "\n",
    "##### For `t` = 1 to `N` do:\n",
    "  1. For each arm `k` in `[1, K]` do:\n",
    "     - If `counts[k]` > 0 then:\n",
    "       - `mean_reward[k]` = `values[k]` / `counts[k]`\n",
    "     - Else:\n",
    "       - `mean_reward[k]` = 0\n",
    "\n",
    "  2. For each arm `k` in `[1, K]` do:\n",
    "     - `ucb_value[k]` = `mean_reward[k]` + sqrt((1.5 * log(n)) / `counts[k]`)\n",
    "\n",
    "  3. Select arm `j` with the highest `ucb_value[j]`\n",
    "\n",
    "  4. Pull arm `j` and observe reward `r`\n",
    "\n",
    "  5. Update `counts[j]` = `counts[j]` + 1\n",
    "\n",
    "  6. Update `values[j]` = `values[j]` + `r`\n",
    "\n",
    "#### Output:\n",
    "- `counts` the number of times each arm has been selected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ucb:\n",
    "    def __init__(self, number_of_arms):\n",
    "        self.number_of_arms = number_of_arms\n",
    "        self.arm_counts = np.zeros(number_of_arms)\n",
    "        self.arm_values = np.zeros(number_of_arms)\n",
    "        self.ucb_values = np.zeros(number_of_arms) + np.inf\n",
    "        self.n = 1\n",
    "\n",
    "    def choose_arm(self):\n",
    "        pass\n",
    "    \n",
    "    def update(self, reward, arm_selected):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.arm_counts = np.zeros(number_of_arms)\n",
    "        self.arm_values = np.zeros(number_of_arms)\n",
    "        self.ucb_values = np.zeros(number_of_arms)* np.inf\n",
    "        self.n = 1\n",
    "    \n",
    "    def get_arm_counts(self):\n",
    "        return self.arm_counts.copy()\n",
    "\n",
    "# Parameters\n",
    "number_of_arms = 4\n",
    "true_rewards = np.linspace(0.4, 0.46, number_of_arms)\n",
    "width = np.array([0.2] * number_of_arms)\n",
    "\n",
    "# Run experiment\n",
    "bandit = KArmedBandit(number_of_arms, true_rewards, width)\n",
    "ucb = Ucb(number_of_arms)\n",
    "num_steps = 1000\n",
    "arm_counts_history = []\n",
    "\n",
    "for i in range(num_steps):\n",
    "    j = ucb.choose_arm()\n",
    "    reward = bandit.pull_arm(j)\n",
    "    ucb.update(reward, j)\n",
    "    current_result = ucb.get_arm_counts()\n",
    "    arm_counts_history.append(current_result)\n",
    "print(f\"Arm counters: {ucb.get_arm_counts()}\")\n",
    "print(f\"Arm values: {ucb.arm_values}\")\n",
    "print(f\"Rewards collected: {bandit.cumulative_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your result\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(len(arm_counts_history)), arm_counts_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Compare UCB, Epsilon-greedy and a random policy. Simulate each strategy with 5 runs for 100, 1000, 10000 and 100000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Advanced Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Provide classic DQN for an atari problem\n",
    "2 Fix the problem of moving target with a second network \n",
    "3 Fix the iid problem using a replay buffer\n",
    "4 Fix the Q overtimation by distinguishing action selection and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approximate Q-Learning \n",
    "\n",
    "Approximate Q-learning extends traditional Q-learning to handle environments with large state-action spaces by using function approximators, such as neural networks, to estimate the Q-function. This approach is essential when dealing with complex environments where maintaining a lookup table for all state-action pairs is impractical.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. **Initialize** $\\theta$ randomly.\n",
    "2. **Reset** environment to $s_0$.\n",
    "\n",
    "3. **For** $k = 1$ to $K$, do:\n",
    "   - **Select** $a_{k-1} = \\arg\\max_{a \\in \\mathcal{A}} Q_{\\theta_{k-1}}(s_{k-1}, a)$ with a probability (1-$\\epsilon$) and a random action with probability $\\epsilon$.\n",
    "   - **Take** action $a_{k-1}$ and **observe** $r_{k-1}$ and $s_{k}$.\n",
    "   - **Let** $(s, a, r, s') = (s_{k-1}, a_{k-1}, r_{k-1}, s_k)$.\n",
    "   - **Compute** $\\delta_k = r + \\gamma \\max_{a' \\in \\mathcal{A}} Q_{\\theta_{k-1}}(s', a') - Q_{\\theta_{k-1}}(s, a)$.\n",
    "   - **Update** $\\theta_{k} = \\theta_{k-1} + \\alpha_k \\delta_k \\nabla_{\\theta} Q_{\\theta_{k-1}}(s, a)$.\n",
    "\n",
    " ## Question 1\n",
    " Complete the following code of the DQN algorithm in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64,)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.99, lr=0.001):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.q_network = QNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "\n",
    "    def act(self, state, inference=False, epsilon=0.1):\n",
    "        if inference:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.q_network(state)\n",
    "            return q_values.argmax().item()\n",
    "        else:\n",
    "            if random.random() < epsilon:\n",
    "                return random.choice(range(self.action_size))\n",
    "            else:\n",
    "                state = torch.FloatTensor(state).unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    q_values = self.q_network(state)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, teminated):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        teminated = torch.FloatTensor([teminated])\n",
    "\n",
    "        # Compute the target Q value\n",
    "        with torch.no_grad():\n",
    "            max_next_q_value = self.q_network(next_state).max(1)[0]\n",
    "            target_q_value = reward + (1 - teminated) * self.gamma * max_next_q_value\n",
    "\n",
    "        # Get the current Q value\n",
    "        current_q_value = \n",
    "\n",
    "        # Compute the loss\n",
    "        loss = \n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code\n",
    "\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_agent(agent, env, episodes=100):\n",
    "    total_reward = 0\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state, inference=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "    return total_reward / episodes\n",
    "\n",
    "def train_agent(agent, env, episodes=250, max_steps=200):\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        for _ in range(max_steps):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Test before training\n",
    "print(\"Testing before training:\")\n",
    "mean_reward_before = test_agent(agent, env)\n",
    "print(f\"Mean Reward before training: {mean_reward_before}\")\n",
    "\n",
    "# Train the agent\n",
    "train_agent(agent, env, episodes=300)\n",
    "\n",
    "# Test after training\n",
    "print(\"Testing after training:\")\n",
    "mean_reward_after = test_agent(agent, env)\n",
    "print(f\"Mean Reward after training: {mean_reward_after}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2. Non-Stationarity (Moving Target Problem)\n",
    "\n",
    "Problem: The target Q-values in approximate Q-learning are constantly changing as the Q-function is updated, leading to a non-stationary target. This makes the learning process unstable and can cause divergence.\n",
    "\n",
    "Solution: Use a target network that is updated less frequently than the main Q-network. This provides a fixed target for a period, stabilizing the learning process.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. **Initialize** $\\theta$ randomly. \n",
    "2. **Copy** $\\theta$ to $\\theta^{\\prime}$ (target network).\n",
    "3. **Reset** environment to $s_0$.\n",
    "\n",
    "4. **For** $k = 1$ to $K$, do:\n",
    "   - **Select** $a_{k-1} = \\arg\\max_{a \\in \\mathcal{A}} Q_{\\theta_{k-1}}(s_{k-1}, a)$ with a probability (1-$\\epsilon$) and a random action with probability $\\epsilon$.\n",
    "   - **Take** action $a_{k-1}$ and **observe** $r_{k-1}$ and $s_{k}$.\n",
    "   - **Let** $(s, a, r, s') = (s_{k-1}, a_{k-1}, r_{k-1}, s_k)$.\n",
    "   - **Compute** $\\delta_k = r + \\gamma \\max_{a' \\in \\mathcal{A}} Q_{\\theta^{\\prime}}(s', a') - Q_{\\theta_{k-1}}(s, a)$.\n",
    "   - **Update** $\\theta_{k} = \\theta_{k-1} + \\alpha_k \\delta_k \\nabla_{\\theta} Q_{\\theta_{k-1}}(s, a)$.\n",
    "   - **Periodically update**  $\\theta^{\\prime}$ with the $\\theta_k$ .\n",
    "\n",
    "Update the Approximate Q-learning algorithm using a using target network. Start from the previous implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Non IID samples (correlated samples).\n",
    "\n",
    "Problem: The agent's experiences are sequentially correlated because they are collected from consecutive time steps in the environment. This violates the assumption of independent and identically distributed (IID) samples, which is a common requirement for many machine learning algorithms, including those used to train neural networks. The lack of IID samples can lead to inefficient learning and instability in the training process.\n",
    "\n",
    "Solution: Use a technique called experience replay. Experience replay involves storing the agent's experiences in a replay buffer and sampling random mini-batches of experiences from this buffer to update the Q-network. This approach helps to break the correlation between consecutive experiences and provides a more diverse set of training samples, which can lead to more stable and efficient learning.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. **Initialize** $\\theta$ randomly.\n",
    "2. **Initialize** empty buffer $\\mathcal{B}$\n",
    "3. **Reset** environment to $s_0$.\n",
    "\n",
    "3. **For** $k = 1$ to $K$, do:\n",
    "   - **Select** $a_{k-1} = \\arg\\max_{a \\in \\mathcal{A}} Q_{\\theta_{k-1}}(s_{k-1}, a)$ with a probability (1-$\\epsilon$) and a random action with probability $\\epsilon$.\n",
    "   - **Take** action $a_{k-1}$ and **observe** $r_{k-1}$ and $s_{k}$.\n",
    "   - **Store** transition $(s_{k-1}, a_{k-1}, r_{k-1}, s_k)$ in $\\mathcal{B}$.\n",
    "   - **Sample** transition $(s, a, r, s')$ from $\\mathcal{B}$.\n",
    "   - **Compute** $\\delta_k = r + \\gamma \\max_{a' \\in \\mathcal{A}} Q_{\\theta_{k-1}}(s', a') - Q_{\\theta_{k-1}}(s, a)$.\n",
    "   - **Update** $\\theta_{k} = \\theta_{k-1} + \\alpha_k \\delta_k \\nabla_{\\theta} Q_{\\theta_{k-1}}(s, a)$.\n",
    "\n",
    "Implement the approximate Q-learning algorithm with a replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Q-value Overestimation (overestimation of maximum sampling)\n",
    "\n",
    "Problem : The max operator used in the update rule can lead to overestimation of action values. This happens because the same Q-values are used both to select and evaluate actions, which can introduce a bias. This overestimation can degrade the performance of the learning algorithm.\n",
    "\n",
    "Solution: Double Q-learning addresses the overestimation problem by decoupling the action selection from the action evaluation. This is achieved by maintaining two separate Q-value estimates and using one set of estimates to determine the best action and the other to evaluate that action. This approach reduces the bias introduced by the max operator.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. **Initialize** $\\theta$ randomly. \n",
    "2. **Copy** $\\theta$ to $\\theta^{\\prime}$ (target network).\n",
    "3. **Reset** environment to $s_0$.\n",
    "\n",
    "4. **For** $k = 1$ to $K$, do:\n",
    "   - **Select** $a_{k-1} = \\arg\\max_{a \\in \\mathcal{A}} Q_{\\theta_{k-1}}(s_{k-1}, a)$ with a probability (1-$\\epsilon$) and a random action with probability $\\epsilon$.\n",
    "   - **Take** action $a_{k-1}$ and **observe** $r_{k-1}$ and $s_{k}$.\n",
    "   - **Let** $(s, a, r, s') = (s_{k-1}, a_{k-1}, r_{k-1}, s_k)$.\n",
    "   - **Compute** $\\delta_k = r + \\gamma \\max_{a' \\in \\mathcal{A}} Q_{\\theta_{k-1}}(s', \\argmax_{a' \\in \\mathcal{A}} Q_{\\theta^{\\prime}_(s', a')}) - Q_{\\theta_{k-1}}(s, a)$.\n",
    "   - **Update** $\\theta_{k} = \\theta_{k-1} + \\alpha_k \\delta_k \\nabla_{\\theta} Q_{\\theta_{k-1}}(s, a)$.\n",
    "   - **Periodically update**  $\\theta^{\\prime}$ with the $\\theta_k$.\n",
    "\n",
    "Implement the approximate Q-learning algorithm with a double Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
