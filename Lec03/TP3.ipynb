{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 INFO8003\n",
    "The idea behind this notebook is to get familiar with RL algorithms related to continuous domain. In this notebook we focus on the fitted-Q algorithm and the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We describe the domain below:\n",
    "\n",
    "- **State space**: $S = \\{(p,v) \\in \\mathbb{R}^2 | |p| \\leq 1, |v| \\leq 3 \\}$ and a *terminal state*. A terminal state can be seen as a regular state in which the system is stuck and for which all the future rewards obtained in the aftermath are zero.\n",
    "    - A terminal state is reached if $|p_{t+1}| > 1$ or $|v_{t+1}| > 3$. \n",
    "\n",
    "- **Action space**: $ A = \\{4,-4\\}$.\n",
    "- **Dynamics**: $\\dot{p} = v$, $\\dot{v} =  \\frac{a}{m (1+Hill^\\prime(p)^2)} - \\frac{g Hill^\\prime(p)}{1+Hill^\\prime(p)^2} - \\frac{s^2 Hill^{\\prime}(p) Hill^{\\prime \\prime}(p) }{1+Hill^\\prime(p)^2}$,  \n",
    "    where $m = 1$, $g = 9.81$ and\n",
    "\n",
    "  $$\n",
    "  Hill(p) = \n",
    "  \\begin{cases}  \n",
    "    p^2 + p & \\text{if} \\quad p < 0 \\\\\n",
    "    \\frac{p}{\\sqrt{1+5p^2}} & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "    - The discrete-time dynamics is obtained by discretizing the time with the time between $t$ and $t+1$ chosen equal to $0.1s$.\n",
    "- **Integration time step**: $0.001$.   \n",
    "- **Reward signal**: \n",
    "  $$\n",
    "  r(p_t,v_t,a_t) = \n",
    "  \\begin{cases} \n",
    "    -1 & \\text{if} \\quad p_{t+1} < -1 \\quad \\text{or} \\quad |v_{t+1}| > 3 \\\\\n",
    "    1 & \\text{if} \\quad p_{t+1} > 1 \\quad \\text{and} \\quad |v_{t+1}| \\le 3 \\\\\n",
    "    0 & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Discount factor**: $\\gamma = 0.95$.\n",
    "- **Time horizon**: $T \\rightarrow +\\infty$.\n",
    "- **Initial state**: $p_0 \\sim \\mathcal{U}(\\left[-0.1, 0.1 \\right])$, $v_0 = 0$.\n",
    "\n",
    "This domain is a *car on the hill* problem, and will be referred to by this name from now on. The figure here below shows an illustration of the domain.\n",
    "\n",
    "<p align=\"center\"> \n",
    "    <img src=\"caronthehill_display.jpeg\" alt=\"Display of the position $p=0$ and the speed $s=1$ of the car.\">\n",
    "    </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of this domain has already been implemented for you to answer the following questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "import imageio\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from typing import Optional, Tuple, Union\n",
    "from display_caronthehill import save_caronthehill_image\n",
    "\n",
    "\n",
    "class CarOnHillEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Car on Hill environment following the Gymnasium interface.\n",
    "    \n",
    "    State space: position [-1, 1], velocity [-3, 3]\n",
    "    Action space: {-4, 4}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, render_mode: Optional[str] = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)  # 0: -4, 1: 4\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([-1.0, -3.0]),\n",
    "            high=np.array([1.0, 3.0]),\n",
    "            dtype=np.float64\n",
    "        )\n",
    "        \n",
    "        # Physics parameters\n",
    "        self.dt = 0.001\n",
    "        self.m = 1.0\n",
    "        self.g = 9.81\n",
    "        \n",
    "        # Initial state bounds\n",
    "        self.initial_position_range = (-0.1, 0.1)\n",
    "        self.initial_velocity = 0.0\n",
    "        \n",
    "        # Discount factor\n",
    "        self.gamma = 0.95\n",
    "        \n",
    "        # Initialize state\n",
    "        self.state = None\n",
    "        self.steps = 0\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.frames = []\n",
    "\n",
    "        \n",
    "\n",
    "    def _hill_function(self, p: float) -> float:\n",
    "        if p < 0:\n",
    "            return p**2 + p\n",
    "        return p / np.sqrt(1 + 5 * p**2)\n",
    "\n",
    "    def _hill_derivative(self, p: float) -> float:\n",
    "        if p < 0:\n",
    "            return 2 * p + 1\n",
    "        return 1 / (1 + 5 * p**2)**(3/2)\n",
    "\n",
    "    def _hill_second_derivative(self, p: float) -> float:\n",
    "        if p < 0:\n",
    "            return 2\n",
    "        return -15 * p / (1 + 5 * p**2)**(5/2)\n",
    "\n",
    "    def _dynamics(self, p: float, v: float, a: float) -> Tuple[float, float]:\n",
    "        \"\"\"Simulate dynamics for one time step (0.1s) using Euler integration.\"\"\"\n",
    "        steps = int(0.1 / self.dt)\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            hill_deriv = self._hill_derivative(p)\n",
    "            hill_second = self._hill_second_derivative(p)\n",
    "        \n",
    "            v_dot = (a / (self.m * (1 + hill_deriv**2)) - \n",
    "                    (self.g * hill_deriv) / (1 + hill_deriv**2) - \n",
    "                    (v**2 * hill_deriv * hill_second) / (1 + hill_deriv**2))\n",
    "            \n",
    "            p += v * self.dt\n",
    "            v += v_dot * self.dt\n",
    "        \n",
    "        return p, v\n",
    "\n",
    "    def _get_reward(self, next_p: float, next_v: float) -> float:\n",
    "        if next_p < -1 or abs(next_v) > 3:\n",
    "            return -1\n",
    "        elif next_p > 1 and abs(next_v) <= 3:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def _is_terminal(self, p: float, v: float) -> bool:\n",
    "        return abs(p) > 1 or abs(v) > 3\n",
    "\n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None) -> Tuple[np.ndarray, dict]:\n",
    "        super().reset(seed=seed)\n",
    "        self.frames = []\n",
    "        p = self.np_random.uniform(*self.initial_position_range)\n",
    "        v = self.initial_velocity\n",
    "        \n",
    "        self.state = np.array([p, v], dtype=np.float32)\n",
    "        self.steps = 0\n",
    "        \n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, dict]:\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        \n",
    "        force = 4 if action == 1 else -4\n",
    "        p, v = self.state\n",
    "        next_p, next_v = self._dynamics(p, v, force)\n",
    "        next_state = np.array([next_p, next_v], dtype=np.float32)\n",
    "        \n",
    "        reward = self._get_reward(next_p, next_v)\n",
    "        terminated = self._is_terminal(next_p, next_v)\n",
    "        truncated = False  # Infinite time horizon\n",
    "        \n",
    "        self.state = next_state\n",
    "        self.steps += 1\n",
    "        if self.render_mode == \"gif\":\n",
    "            self.render(next_p, next_v)\n",
    "        return next_state, reward, terminated, truncated, {}\n",
    "    \n",
    "    def render(self, position: float, velocity: float):\n",
    "        \"\"\"Render the current state of the environment.\"\"\"\n",
    "        if self.render_mode == \"gif\":\n",
    "            frame = save_caronthehill_image(position, max(min(velocity, 3), -3))\n",
    "            self.frames.append(frame)\n",
    "\n",
    "\n",
    "    def save_gif(self, filename=\"car_on_hill.gif\"):\n",
    "        \"\"\"Save the collected frames as a GIF.\"\"\"\n",
    "        if self.render_mode == \"gif\" and self.frames:\n",
    "            imageio.mimsave(filename, self.frames, fps=10)\n",
    "            print(f\"GIF saved as {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can render a trajectory using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved as car_on_hill.gif\n"
     ]
    }
   ],
   "source": [
    "env = CarOnHillEnv(render_mode=\"gif\")\n",
    "\n",
    "\n",
    "num_steps = 100\n",
    "state, _ = env.reset()\n",
    "for _ in range(num_steps):\n",
    "    action = env.action_space.sample() # We implement a random policy here\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    state = next_state\n",
    "\n",
    "env.save_gif(\"car_on_hill.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Fitted Q iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Fitted Q Iteration Algorithm\n",
    "\n",
    "Implement the Fitted-Q-Iteration algorithm for the car on the hill environment. It should use a sklearn model for the regression algorithm. Propose two stopping rules for the computation of the $\\widehat{Q}_N$-functions sequence and motivate them.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "#### Inputs:\n",
    "- A set of four-tuples $\\mathcal{F}$ (experience replay buffer)\n",
    "- A regression algorithm\n",
    "\n",
    "#### Initialization:\n",
    "- Set $N$ to 0.\n",
    "- Let $\\hat{Q}_N$ be a function equal to zero everywhere on $\\mathcal{S} \\times \\mathcal{A}$.\n",
    "\n",
    "#### Iterations:\n",
    "Repeat until stopping conditions are reached\n",
    "\n",
    "1. **Increment Iteration Counter:**\n",
    "   - $N \\leftarrow N + 1$.\n",
    "\n",
    "2. **Build the Training Set:**\n",
    "   - Construct the training set $\\mathcal{TS} = \\{(i^l, o^l)\\}, l = 1, \\ldots, \\#\\mathcal{F}$ based on the function $\\hat{Q}_{N-1}$ and on the full set of four-tuples $\\mathcal{F}$:\n",
    "     \\[\n",
    "     \\begin{aligned}\n",
    "     i^l &= (s^l, a^l), \\\\\n",
    "     o^l &= r^l + \\gamma \\max_{a' \\in \\mathcal{A}} \\hat{Q}_{N-1}(s^l_{+1}, a')\n",
    "     \\end{aligned}\n",
    "     \\]\n",
    "\n",
    "3. **Induce the Function:**\n",
    "   - Use the regression algorithm to induce from $\\mathcal{TS}$ the function $\\hat{Q}_N(s, a)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the implementation here bellow as a starting point\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "class FittedQIteration:\n",
    "    def __init__(self, model, gamma: float, action_space: List[int]):\n",
    "        \"\"\"\n",
    "        Initialize the Fitted Q-Iteration algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - model: A regression model from scikit-learn used to approximate the Q-function.\n",
    "        - gamma: The discount factor for future rewards.\n",
    "        - action_space: A list of possible actions in the environment.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.action_space = action_space\n",
    "        self.q_function = None\n",
    "\n",
    "    def train(self, experience_replay: List[Tuple[np.ndarray, int, float, np.ndarray]], stopping_criteria: str):\n",
    "        \"\"\"\n",
    "        Train the Q-function using the Fitted Q-Iteration algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - experience_replay: A list of experience tuples (state, action, reward, next_state).\n",
    "        - stopping_criteria: The criteria to stop training.\n",
    "        \"\"\"\n",
    "        # Initialize Q function to zero\n",
    "        if self.q_function is None:\n",
    "            self.q_function = lambda s, a: 0\n",
    "\n",
    "        inputs = []\n",
    "        targets = []\n",
    "\n",
    "       \n",
    "    def predict_Q(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the Q-values for all actions given a state.\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current state for which to predict Q-values.\n",
    "\n",
    "        Returns:\n",
    "        - An array of Q-values for each action in the action space.\n",
    "        \"\"\"\n",
    "        return np.array([self.q_function(state, a) for a in self.action_space])\n",
    "    \n",
    "    def predict_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Predict the best action for a given state based on the Q-function.\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current state for which to predict the best action.\n",
    "\n",
    "        Returns:\n",
    "        - The action with the highest Q-value.\n",
    "        \"\"\"\n",
    "        return np.argmax(np.array([self.q_function(state, a) for a in self.action_space]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Generating Sets of One-Step System Transitions\n",
    "\n",
    "Propose two strategies for generating sets of one-step system transitions and motivate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Use the following supervised learning techniques:\n",
    "- Linear Regression\n",
    "- Extremely Randomized Trees\n",
    "- Neural Networks\n",
    "\n",
    "Build and motivate your neural network structure.\n",
    "These techniques are implemented in the `scikit-learn` libraries.\n",
    "Derive the policy $\\widehat{\\mu}_*$ from $\\widehat{Q}$ and display the Q-values and the policy in a colored 2D grid. Use red for action a = -4 and blue for action a = 4, with a resolution of 0.01 for the state space display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import numpy as np\n",
    "\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"MLPRegressor\": MLPRegressor(),\n",
    "    \"ExtraTreesRegressor\": ExtraTreesRegressor() # Hint: make some quick research in the litterature to find interesting parameters\n",
    "}\n",
    "\n",
    "# Define stopping conditions\n",
    "stopping_conditions = []\n",
    "\n",
    "# Define tuple generation techniques\n",
    "tuple_generation_techniques = []\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "# Train FQI for each combination\n",
    "for model_name, model in models.items():\n",
    "    for stopping_condition in stopping_conditions:\n",
    "        for technique in tuple_generation_techniques:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play the policy\n",
    "\n",
    "env = CarOnHillEnv(render_mode=\"gif\")\n",
    "\n",
    "num_steps = 100\n",
    "state, _ = env.reset()\n",
    "for _ in range(num_steps):\n",
    "    action = your_model.predict(state)\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "    state = next_state\n",
    "\n",
    "env.save_gif(\"car_on_hill.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Estimate and Display Expected Return\n",
    "\n",
    "Estimate and display the expected return of $\\widehat{\\mu}_N^*$ in a table for each:\n",
    "  - Supervised learning algorithm.\n",
    "  - One-step system transitions generation strategy.\n",
    "  - Stopping rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Results Discussion\n",
    "\n",
    "Discuss the impact on the results for each:\n",
    "- Supervised learning algorithm.\n",
    "- One-step system transitions generation strategies.\n",
    "- Stopping rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parametric Q-Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Question 1: Parametric Q-Learning Algorithm\n",
    "\n",
    "Implement a routine which computes a parametrized approximation of the Q-function via the Parametric Q-Learning algorithm. Use a neural network as the approximation architecture, and motivate its structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from typing import List, Tuple\n",
    "\n",
    "class ParametricQLearning:\n",
    "    def __init__(self, model, gamma: float, action_space: List[int], learning_rate: float = 0.01):\n",
    "        \"\"\"\n",
    "        Initialize the Parametric Q-Learning algorithm.\n",
    "\n",
    "        Parameters:\n",
    "        - model: A scikit-learn model used for approximating the Q-function.\n",
    "        - gamma: Discount factor for future rewards.\n",
    "        - action_space: List of possible actions.\n",
    "        - learning_rate: Learning rate for updating the Q-function.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.action_space = action_space\n",
    "        self.learning_rate = learning_rate\n",
    "        # Initialize Q-function as a lambda function\n",
    "        self.q_function = lambda s, a: self.model.predict([np.append(s, a)])[0]\n",
    "\n",
    "    def train(self, env, num_episodes: int, max_steps: int):\n",
    "        \"\"\"\n",
    "        Train the Q-learning model over a number of episodes.\n",
    "\n",
    "        Parameters:\n",
    "        - env: The environment to train on.\n",
    "        - num_episodes: Number of episodes to train for.\n",
    "        - max_steps: Maximum number of steps per episode.\n",
    "        \"\"\"\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Select an action using an epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        - An action from the action space.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def update_q_function(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray):\n",
    "        \"\"\"\n",
    "        Update the Q-function using the Bellman equation.\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current state.\n",
    "        - action: The action taken.\n",
    "        - reward: The reward received.\n",
    "        - next_state: The next state after taking the action.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def predict_action(self, state: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Predict the best action for a given state.\n",
    "\n",
    "        Parameters:\n",
    "        - state: The current state.\n",
    "\n",
    "        Returns:\n",
    "        - The action with the highest Q-value.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Policy Derivation and Visualization\n",
    "\n",
    "Derive the policy $\\widehat{\\mu}_*$ from $\\widehat{Q}$ and display it in a colored 2D grid. Use red for action a = -4 and blue for action a = 4, with a resolution of 0.01 for the state space display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Expected Return Estimation\n",
    "\n",
    "Estimate and show the expected return of $\\widehat{\\mu}^*$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Experimental Protocol Design\n",
    "\n",
    "Design an experimental protocol to compare Fitted Q Iteration (FQI) and Parametric Q-Learning. Use a curve plot where the x-axis represents the number of one-step system transitions and the y-axis represents the expected return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5: Results Discussion\n",
    "\n",
    "Discuss the results obtained by running the experimental protocol. Consider the differences in performance between FQI and Parametric Q-Learning, and any insights gained from the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
